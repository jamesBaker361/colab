{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamesBaker361/colab/blob/master/3_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uTWz_yaYYiRE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LDKbZuZDblaF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FFh9yLBnddJn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D_ea2oppeEK1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Dataset"
      ]
    },
    {
      "metadata": {
        "id": "dLVgH7ReeH-U",
        "colab_type": "code",
        "outputId": "d71a54da-7165-4110-df47-ca8b64b5578e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#get data!!!\n",
        "\n",
        "path_to_file=tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FK6gXIuneXmT",
        "colab_type": "code",
        "outputId": "888fb0a8-f704-4298-fe55-0f9affb1a5e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "#convert her to array\n",
        "text=open(path_to_file,\"rb\").read().decode(encoding='utf-8')\n",
        "print(text[5000:5500])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "or heart, the arm our soldier,\n",
            "Our steed the leg, the tongue our trumpeter.\n",
            "With other muniments and petty helps\n",
            "In this our fabric, if that they--\n",
            "\n",
            "MENENIUS:\n",
            "What then?\n",
            "'Fore me, this fellow speaks! What then? what then?\n",
            "\n",
            "First Citizen:\n",
            "Should by the cormorant belly be restrain'd,\n",
            "Who is the sink o' the body,--\n",
            "\n",
            "MENENIUS:\n",
            "Well, what then?\n",
            "\n",
            "First Citizen:\n",
            "The former agents, if they did complain,\n",
            "What could the belly answer?\n",
            "\n",
            "MENENIUS:\n",
            "I will tell you\n",
            "If you'll bestow a small--of what you have li\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MIZXeH8ge5e4",
        "colab_type": "code",
        "outputId": "abafbe29-99a8-421a-816a-07ce072a0915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# the unique characters that appear\n",
        "vocab=sorted(set(text))\n",
        "print(vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w4_GQP7MfKsi",
        "colab_type": "code",
        "outputId": "93595446-b1f6-46a6-d3f2-b5f59cf64d2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v5EBEowCffuP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#lets map from chars to ints\n",
        "char2idx={u:i for i,u in enumerate(vocab)}\n",
        "idx2char=np.array(vocab)\n",
        "text_as_int=np.array([char2idx[x] for x in text])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CJEJ-eRIf9ab",
        "colab_type": "code",
        "outputId": "c9f15e0b-195e-425c-8dcb-3491b6516157",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(text_as_int)\n",
        "print(\"{}----- Characters mapped to int---> {}\".format(repr(text[500:520]), text_as_int[500:520]) )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[18 47 56 ... 45  8  0]\n",
            "' citizens, the patri'----- Characters mapped to int---> [ 1 41 47 58 47 64 43 52 57  6  1 58 46 43  1 54 39 58 56 47]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8hO7b7TfgdIK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#creating learnable datasex\n",
        "#hyperparamters\n",
        "#using last seq_length numbers, we try to predict the seq_length+1th character\n",
        "\n",
        "seq_length=150\n",
        "examples_per_epoch=len(text)//seq_length\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y_0vsrQ8hEGy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#now we need to make our data into a tensor\n",
        "char_dataset=tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "sequences=char_dataset.batch(seq_length+1,drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cx1Hbw0xh0pr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def split_input_target(block):\n",
        "  input_text=block[:-1]\n",
        "  target_text=block[1:]\n",
        "  return input_text,target_text\n",
        "\n",
        "dataset=sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N4M1agA6iiai",
        "colab_type": "code",
        "outputId": "6c93e202-7123-4580-dd9a-399388494ca2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "for in_example, tar_example in dataset.take(3):\n",
        "  print(\"Input data: \",repr(''.join(idx2char[in_example.numpy()])))\n",
        "  print(\"Target data: \",repr(''.join(idx2char[tar_example.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nA'\n",
            "Target data:  'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAl'\n",
            "Input data:  \"l:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us \"\n",
            "Target data:  \":\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us k\"\n",
            "Input data:  \"ill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good\"\n",
            "Target data:  \"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good \"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N2ekS37kjnZ8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#more hyperparameters\n",
        "BATCH_SIZE=250\n",
        "steps_per_epoch=examples_per_epoch//BATCH_SIZE\n",
        "BUFFER_SIZE=10000 \n",
        "#rnns need buffer in case they turn infinite \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wZgh19oGkIyf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset=dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xGJzGIsAkXPK",
        "colab_type": "code",
        "outputId": "71e80067-3fab-4ea2-90c1-8d64c1aecf31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: ((250, 150), (250, 150)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "MmYIuuKPkm8X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Model"
      ]
    },
    {
      "metadata": {
        "id": "R4N_qzTMkogJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#more hyperparamters\n",
        "vocab_size=len(vocab) #size inout and output\n",
        "embedding_dim=256 \n",
        "rnn_units=1024\n",
        "\n",
        "if tf.test.is_gpu_available():\n",
        "  rnn=tf.keras.layers.CuDNNGRU #could also do CUDNNLSTM?\n",
        "else:\n",
        "  import functools\n",
        "  rnn=functools.partial(tf.keras.layers.GRY, recurrent_activation=\"sigmoid\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N1bWmoVWmGqw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "nxA-Cd6zm85M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size,embedding_dim,rnn_units,batch_size):\n",
        "  model=tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "      rnn(rnn_units, return_sequences=True, recurrent_initializer=\"glorot_uniform\", stateful=True), #recurrent_initializer just initializes with less aggy weights\n",
        "      tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "59b0Du6Koo6l",
        "colab_type": "code",
        "outputId": "c29bdda3-6964-46d5-eeff-f0d5c71c4dea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "model=build_model(\n",
        "  vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (250, None, 256)          16640     \n",
            "_________________________________________________________________\n",
            "cu_dnngru (CuDNNGRU)         (250, None, 1024)         3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (250, None, 65)           66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f3OuvYQZqRpZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Loss"
      ]
    },
    {
      "metadata": {
        "id": "9aFt7S2NqS0D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss(labels,logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels,logits, from_logits=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2fOqHLzXqkng",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wwbt-1jBqu9v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Training Time"
      ]
    },
    {
      "metadata": {
        "id": "UVK2Tiuxq5xv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "optimizer=tf.train.AdamOptimizer(),\n",
        "loss=loss)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s8Ng9BR5roXT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#we gonna use some callbacks? So we can save stuff\n",
        "checkpoint_dir=\"./training_checkpoints\"\n",
        "checkpoint_prefix=os.path.join(checkpoint_dir,'ckpt_{epoch}')\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "filepath=checkpoint_prefix,\n",
        "save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rL2ffb7nspZ-",
        "colab_type": "code",
        "outputId": "cb4d31f5-c4f9-4590-9353-ee077c0761a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "history=model.fit(\n",
        "dataset.repeat(),\n",
        "epochs=10,\n",
        "steps_per_epoch=steps_per_epoch,\n",
        "callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "29/29 [==============================] - 19s 649ms/step - loss: 1.8481\n",
            "Epoch 2/10\n",
            "29/29 [==============================] - 19s 668ms/step - loss: 1.7779\n",
            "Epoch 3/10\n",
            "29/29 [==============================] - 19s 666ms/step - loss: 1.7155\n",
            "Epoch 4/10\n",
            "29/29 [==============================] - 19s 663ms/step - loss: 1.6601\n",
            "Epoch 5/10\n",
            "29/29 [==============================] - 19s 663ms/step - loss: 1.6097\n",
            "Epoch 6/10\n",
            "29/29 [==============================] - 19s 664ms/step - loss: 1.5674\n",
            "Epoch 7/10\n",
            "29/29 [==============================] - 19s 662ms/step - loss: 1.5297\n",
            "Epoch 8/10\n",
            "29/29 [==============================] - 19s 662ms/step - loss: 1.4961\n",
            "Epoch 9/10\n",
            "29/29 [==============================] - 19s 661ms/step - loss: 1.4650\n",
            "Epoch 10/10\n",
            "29/29 [==============================] - 20s 677ms/step - loss: 1.4388\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ux-H9kliuYtg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HRqDsOF4uZJC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Run"
      ]
    },
    {
      "metadata": {
        "id": "_FAyzH4ouaGn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model=build_model(vocab_size,embedding_dim,rnn_units,batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1,None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s_6gULlhwkYp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_text(model,start_string=\" \",num_generate=1000,temperature=0.5):\n",
        "  num_generate=num_generate\n",
        "  \n",
        "  input_eval=[char2idx[s] for s in start_string]\n",
        "  input_eval=tf.expand_dims(input_eval,0)\n",
        "  \n",
        "  text_generated=[]\n",
        "  temperature=temperature\n",
        "  \n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions=model(input_eval)\n",
        "    predictions=tf.squeeze(predictions,0)\n",
        "    \n",
        "    predictions=predictions/temperature\n",
        "    predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
        "    \n",
        "    input_eval=tf.expand_dims([predicted_id],0)\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "    \n",
        "  return(start_string+\"\".join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R-ZIxaPEx3TU",
        "colab_type": "code",
        "outputId": "52502bca-1546-4a92-87cd-5f1b3f441062",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "cell_type": "code",
      "source": [
        "print(generate_text(model,start_string=u\"I Want to Fuck Juliet\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I Want to Fuck Juliet.\n",
            "\n",
            "CLAUDIO:\n",
            "What is your noble crowns, that thou diest it do not here it\n",
            "shall be some and the repting to the king and your heart,\n",
            "Where is the end then he did be well of men.\n",
            "\n",
            "CORIOLANUS:\n",
            "Ay, that say. I sear my some stare\n",
            "To see the matter with the first of the posest you,\n",
            "Why then the prince that thou shalt bear the breasted borns and here,\n",
            "What shall I would the will and man say is here.\n",
            "\n",
            "CLARENCE:\n",
            "A gentleman arred by the suncess of this face with him.\n",
            "\n",
            "LUCIO:\n",
            "I did you thou desters the world it with a grave it doth the earth to the people.\n",
            "\n",
            "BRUTUS:\n",
            "Nor I am possess, that you shall be so loth, and the senden of his head,\n",
            "And praise you have heard the senden heart, even ere the born that he had fair and soul,\n",
            "When all the bentle stringer to the seace to remember him place and the heart,\n",
            "Which thou thou art a face of this farse and so heard\n",
            "The son, the give the metter of the castless hand,\n",
            "And there is the very sid and and the more than with him all the courtes,\n",
            "And the that the tr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vp3oxHhCzBm2",
        "colab_type": "code",
        "outputId": "7942e979-e292-48b6-c340-b3625429090c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "cell_type": "code",
      "source": [
        "print(generate_text(model,start_string=u\"I Want to Fuck Romeo\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I Want to Fuck Romeo.\n",
            "\n",
            "MARCIUS:\n",
            "I have been and the norsel of his truth, by this king\n",
            "And ever thou hast that do not to the head\n",
            "Of the time of men and the fired than the prince\n",
            "And lives the cloud of the sender them all.\n",
            "\n",
            "CORIOLANUS:\n",
            "The time that he shall be here?\n",
            "\n",
            "CLARENCE:\n",
            "What think the seas that we have then and love shall be the now all my course.\n",
            "\n",
            "KING HENRY VI:\n",
            "What is the bring of the mother words and leave to me;\n",
            "Madam, to proce the county talker is a noble shall be stands that he will be senden to the corventer\n",
            "Than thee in the crown of charge you than the world.\n",
            "\n",
            "BRANUS:\n",
            "The sent of the more to the princely of the princes,\n",
            "And so her heaven now the highness make the love:\n",
            "And the great breathed with the friend, think your head is the princely said,\n",
            "And my heart the warched with the truth, but the day there he that save your sight,\n",
            "The amed and all the house and the will say the world,\n",
            "To be so well the companous it to the hearth,\n",
            "What stand the senst thou frouse and my soul soul\n",
            "To speak the \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PLHFDNg8yueL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}